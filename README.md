This is a language model implemented from scratch. To run, simply download the repository, modify generator.py with your chosen prompt and run. Feel free to explore with beam search, but do note that it is quite slow.
Also keep in mind that while I am very proud of this model, it is trained on a fraction of the data available to large language models, and its representational power is also limited. For context, GPT-1 had somewhere in the 
neighbourhood of 117 Million parameters, mine has some 75 Million. 

Hope you enjoy!

Resources I used to help build this tool

1. Attention is all you need - Vasvani et. al.
2. https://github.com/lucidrains/x-transformers
3. https://jalammar.github.io/illustrated-transformer/
4. https://jalammar.github.io/illustrated-gpt2/
5. https://www.youtube.com/watch?v=kCc8FmEb1nY
6. https://wingedsheep.com/building-a-language-model/
